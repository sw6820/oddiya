name: Deploy to AWS

on:
  push:
    branches:
      - main
    paths:
      - 'services/**'
      - 'infrastructure/terraform/phase1/**'
      - '.github/workflows/deploy-aws.yml'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy'
        required: true
        default: 'prod'
        type: choice
        options:
          - prod
          - staging

jobs:
  deploy:
    name: Deploy to AWS Seoul
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Java 21
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'corretto'

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-northeast-2

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Create terraform.tfvars
        run: |
          cd infrastructure/terraform/phase1
          cat > terraform.tfvars << EOF
          aws_region = "ap-northeast-2"
          environment = "production2024"
          instance_type = "t2.micro"
          key_pair_name = "${{ secrets.SSH_KEY_NAME }}"
          admin_ip_whitelist = ["${{ secrets.ADMIN_IP }}/32"]
          db_password = "${{ secrets.DB_PASSWORD }}"
          gemini_api_key = "${{ secrets.GEMINI_API_KEY }}"
          google_client_id = "${{ secrets.GOOGLE_CLIENT_ID }}"
          google_client_secret = "${{ secrets.GOOGLE_CLIENT_SECRET }}"
          apple_client_id = "${{ secrets.APPLE_CLIENT_ID }}"
          apple_team_id = "${{ secrets.APPLE_TEAM_ID }}"
          apple_key_id = "${{ secrets.APPLE_KEY_ID }}"
          apple_private_key = "${{ secrets.APPLE_PRIVATE_KEY }}"
          EOF

      - name: Terraform Init
        run: |
          cd infrastructure/terraform/phase1
          terraform init

      - name: Cleanup Existing Resources
        if: github.ref == 'refs/heads/main'
        run: |
          echo "=== Cleaning up leftover resources from previous runs ==="

          # Terminate any leftover EC2 instances with production2024 environment tag
          echo "Checking for leftover EC2 instances..."
          INSTANCE_IDS=$(aws ec2 describe-instances \
            --filters "Name=tag:Environment,Values=production2024" "Name=instance-state-name,Values=running,stopped,stopping" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || echo "")

          if [ -n "$INSTANCE_IDS" ]; then
            echo "Terminating instances: $INSTANCE_IDS"
            aws ec2 terminate-instances --instance-ids $INSTANCE_IDS 2>/dev/null || true
            echo "Waiting 60s for instances to fully terminate and ENIs to detach..."
            sleep 60
          fi

          # Get security group IDs
          APP_SG=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=oddiya-app-server-production2024" --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null || echo "")
          DB_SG=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=oddiya-db-server-production2024" --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null || echo "")

          # Delete network interfaces associated with security groups
          echo "Cleaning up network interfaces..."
          for SG in $DB_SG $APP_SG; do
            if [ -n "$SG" ] && [ "$SG" != "None" ]; then
              ENI_IDS=$(aws ec2 describe-network-interfaces --filters "Name=group-id,Values=$SG" --query 'NetworkInterfaces[*].NetworkInterfaceId' --output text 2>/dev/null || echo "")
              if [ -n "$ENI_IDS" ]; then
                for ENI in $ENI_IDS; do
                  echo "Deleting ENI: $ENI"
                  aws ec2 delete-network-interface --network-interface-id "$ENI" 2>/dev/null || true
                done
                sleep 10
              fi
            fi
          done

          # Delete DB security group first (it depends on app SG)
          if [ -n "$DB_SG" ] && [ "$DB_SG" != "None" ]; then
            echo "Deleting DB security group $DB_SG..."
            # First revoke all rules to remove dependencies
            DB_INGRESS=$(aws ec2 describe-security-groups --group-ids "$DB_SG" --query 'SecurityGroups[0].IpPermissions' --output json 2>/dev/null)
            DB_EGRESS=$(aws ec2 describe-security-groups --group-ids "$DB_SG" --query 'SecurityGroups[0].IpPermissionsEgress' --output json 2>/dev/null)
            [ "$DB_INGRESS" != "[]" ] && aws ec2 revoke-security-group-ingress --group-id "$DB_SG" --ip-permissions "$DB_INGRESS" 2>/dev/null || true
            [ "$DB_EGRESS" != "[]" ] && aws ec2 revoke-security-group-egress --group-id "$DB_SG" --ip-permissions "$DB_EGRESS" 2>/dev/null || true
            sleep 5
            aws ec2 delete-security-group --group-id "$DB_SG" 2>/dev/null && echo "âœ“ Deleted DB SG" || echo "âš  Could not delete DB SG (may still be in use)"
          fi

          # Delete App security group
          if [ -n "$APP_SG" ] && [ "$APP_SG" != "None" ]; then
            echo "Deleting App security group $APP_SG..."
            # Revoke all rules
            APP_INGRESS=$(aws ec2 describe-security-groups --group-ids "$APP_SG" --query 'SecurityGroups[0].IpPermissions' --output json 2>/dev/null)
            APP_EGRESS=$(aws ec2 describe-security-groups --group-ids "$APP_SG" --query 'SecurityGroups[0].IpPermissionsEgress' --output json 2>/dev/null)
            [ "$APP_INGRESS" != "[]" ] && aws ec2 revoke-security-group-ingress --group-id "$APP_SG" --ip-permissions "$APP_INGRESS" 2>/dev/null || true
            [ "$APP_EGRESS" != "[]" ] && aws ec2 revoke-security-group-egress --group-id "$APP_SG" --ip-permissions "$APP_EGRESS" 2>/dev/null || true
            sleep 5
            aws ec2 delete-security-group --group-id "$APP_SG" 2>/dev/null && echo "âœ“ Deleted App SG" || echo "âš  Could not delete App SG (may still be in use)"
          fi

          # Delete IAM resources
          ROLE_NAME="oddiya-app-server-role-production2024"
          PROFILE_NAME="oddiya-app-server-profile-production2024"
          if aws iam get-role --role-name $ROLE_NAME 2>/dev/null; then
            echo "Found IAM role, deleting..."
            for policy in $(aws iam list-attached-role-policies --role-name $ROLE_NAME --query 'AttachedPolicies[*].PolicyArn' --output text 2>/dev/null); do
              aws iam detach-role-policy --role-name $ROLE_NAME --policy-arn $policy 2>/dev/null || true
            done
            aws iam remove-role-from-instance-profile --instance-profile-name $PROFILE_NAME --role-name $ROLE_NAME 2>/dev/null || true
            aws iam delete-instance-profile --instance-profile-name $PROFILE_NAME 2>/dev/null || true
            aws iam delete-role --role-name $ROLE_NAME 2>/dev/null || true
            echo "âœ“ Deleted IAM role"
          fi

          echo "âœ“ Cleanup complete"

      - name: Terraform Plan
        run: |
          cd infrastructure/terraform/phase1
          terraform plan

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main'
        run: |
          cd infrastructure/terraform/phase1
          terraform apply -auto-approve

      - name: Get EC2 IPs
        id: ec2-ips
        run: |
          cd infrastructure/terraform/phase1
          echo "app_ip=$(terraform output -raw app_server_public_ip)" >> $GITHUB_OUTPUT
          echo "db_ip=$(terraform output -raw db_server_private_ip)" >> $GITHUB_OUTPUT

      - name: Wait for Instance and Open SSH
        run: |
          APP_IP="${{ steps.ec2-ips.outputs.app_ip }}"

          # Get security group ID
          APP_SG=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=oddiya-app-server-production2024" --query 'SecurityGroups[0].GroupId' --output text)

          # Temporarily allow SSH from GitHub Actions runners (0.0.0.0/0 for now)
          echo "Opening SSH port for deployment..."
          aws ec2 authorize-security-group-ingress --group-id "$APP_SG" --protocol tcp --port 22 --cidr 0.0.0.0/0 2>/dev/null || echo "Rule already exists or failed"

          # Wait for instance to be fully ready and SSH accessible
          echo "Waiting for instance to be ready..."
          for i in {1..30}; do
            if nc -z -w5 "$APP_IP" 22 2>/dev/null; then
              echo "âœ“ SSH is accessible"
              break
            fi
            echo "Attempt $i/30: Waiting for SSH..."
            sleep 10
          done

      - name: Build Plan Service
        run: |
          cd services/plan-service
          chmod +x gradlew
          ./gradlew clean build -x test

      - name: Package LLM Agent
        run: |
          cd services/llm-agent
          tar czf llm-agent.tar.gz src/ requirements.txt main.py

      - name: Setup SSH Key
        run: |
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > /tmp/oddiya-prod.pem
          chmod 400 /tmp/oddiya-prod.pem

      - name: Deploy Plan Service
        run: |
          APP_IP="${{ steps.ec2-ips.outputs.app_ip }}"

          # Setup directory and service
          ssh -i /tmp/oddiya-prod.pem \
            -o StrictHostKeyChecking=no \
            ec2-user@$APP_IP << 'ENDSSH'
          # Create directories
          sudo mkdir -p /opt/oddiya/plan-service
          sudo chown ec2-user:ec2-user /opt/oddiya/plan-service

          # Create systemd service
          sudo tee /etc/systemd/system/plan-service.service > /dev/null <<'EOF'
          [Unit]
          Description=Plan Service
          After=network.target

          [Service]
          Type=simple
          User=ec2-user
          WorkingDirectory=/opt/oddiya/plan-service
          ExecStart=/usr/bin/java -jar /opt/oddiya/plan-service/app.jar
          Restart=always
          RestartSec=10

          [Install]
          WantedBy=multi-user.target
          EOF

          sudo systemctl daemon-reload
          sudo systemctl enable plan-service
          ENDSSH

          # Copy JAR
          scp -i /tmp/oddiya-prod.pem \
            -o StrictHostKeyChecking=no \
            services/plan-service/build/libs/plan-service-*.jar \
            ec2-user@$APP_IP:/opt/oddiya/plan-service/app.jar

          # Start service
          ssh -i /tmp/oddiya-prod.pem \
            -o StrictHostKeyChecking=no \
            ec2-user@$APP_IP \
            "sudo systemctl restart plan-service"

      - name: Deploy LLM Agent
        run: |
          APP_IP="${{ steps.ec2-ips.outputs.app_ip }}"

          # Setup directory, venv, and service
          ssh -i /tmp/oddiya-prod.pem \
            -o StrictHostKeyChecking=no \
            ec2-user@$APP_IP << 'ENDSSH'
          # Create directories
          sudo mkdir -p /opt/oddiya/llm-agent
          sudo chown ec2-user:ec2-user /opt/oddiya/llm-agent

          # Create venv if it doesn't exist
          if [ ! -d "/opt/oddiya/llm-agent/venv" ]; then
            python3 -m venv /opt/oddiya/llm-agent/venv
          fi

          # Create systemd service
          sudo tee /etc/systemd/system/llm-agent.service > /dev/null <<'EOF'
          [Unit]
          Description=LLM Agent Service
          After=network.target

          [Service]
          Type=simple
          User=ec2-user
          WorkingDirectory=/opt/oddiya/llm-agent
          Environment="PATH=/opt/oddiya/llm-agent/venv/bin"
          ExecStart=/opt/oddiya/llm-agent/venv/bin/python main.py
          Restart=always
          RestartSec=10

          [Install]
          WantedBy=multi-user.target
          EOF

          sudo systemctl daemon-reload
          sudo systemctl enable llm-agent
          ENDSSH

          # Copy package
          scp -i /tmp/oddiya-prod.pem \
            -o StrictHostKeyChecking=no \
            services/llm-agent/llm-agent.tar.gz \
            ec2-user@$APP_IP:/tmp/

          # Create .env file locally with secrets
          cat > /tmp/llm-agent.env << EOF
          GOOGLE_API_KEY=${{ secrets.GEMINI_API_KEY }}
          GEMINI_MODEL=gemini-2.0-flash-exp
          LLM_PROVIDER=gemini
          MOCK_MODE=false
          EOF

          # Copy .env file to server
          scp -i /tmp/oddiya-prod.pem \
            -o StrictHostKeyChecking=no \
            /tmp/llm-agent.env \
            ec2-user@$APP_IP:/tmp/.env

          # Extract and install
          ssh -i /tmp/oddiya-prod.pem \
            -o StrictHostKeyChecking=no \
            ec2-user@$APP_IP << 'ENDSSH'
          cd /opt/oddiya/llm-agent
          tar xzf /tmp/llm-agent.tar.gz

          # Move .env file to correct location
          mv /tmp/.env .env
          chmod 600 .env

          source venv/bin/activate
          pip install -r requirements.txt
          sudo systemctl restart llm-agent
          ENDSSH

      - name: Health Check
        run: |
          APP_IP="${{ steps.ec2-ips.outputs.app_ip }}"

          echo "Waiting for services to start..."
          sleep 30

          echo "Testing LLM Agent..."
          curl -f http://$APP_IP:8000/health || echo "LLM Agent health check failed"

          echo "Testing Plan Service..."
          curl -f http://$APP_IP:8083/actuator/health || echo "Plan Service health check failed"

      - name: Cleanup
        if: always()
        run: |
          rm -f /tmp/oddiya-prod.pem

      - name: Deployment Summary
        run: |
          echo "### Deployment Complete! ðŸš€" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**App Server IP:** ${{ steps.ec2-ips.outputs.app_ip }}" >> $GITHUB_STEP_SUMMARY
          echo "**LLM Agent:** http://${{ steps.ec2-ips.outputs.app_ip }}:8000" >> $GITHUB_STEP_SUMMARY
          echo "**Plan Service:** http://${{ steps.ec2-ips.outputs.app_ip }}:8083" >> $GITHUB_STEP_SUMMARY
