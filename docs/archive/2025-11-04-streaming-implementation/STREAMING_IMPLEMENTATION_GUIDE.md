# Streaming Implementation Guide

**ëª©ì :** ChatGPTì²˜ëŸ¼ ì—¬í–‰ ê³„íšì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¡°ê¸ˆì”© ìŠ¤íŠ¸ë¦¬ë°í•˜ë©´ì„œ ë³´ì—¬ì£¼ê¸°

**í˜„ì¬ ìƒíƒœ:** ì™„ë£Œ í›„ í•œ ë²ˆì— ì „ì²´ ì‘ë‹µ ë°˜í™˜
**ëª©í‘œ ìƒíƒœ:** ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì ì§„ì  ì‘ë‹µ (Server-Sent Events)

---

## ğŸ“Š í˜„ì¬ vs ëª©í‘œ ì•„í‚¤í…ì²˜

### í˜„ì¬ (Non-Streaming)
```
Client â†’ POST /api/v1/plans/generate
  â†“ (loading... 2-5ì´ˆ ëŒ€ê¸°)
Server â†’ {"title": "...", "days": [...]} (ì „ì²´ JSON í•œ ë²ˆì—)
```

### ëª©í‘œ (Streaming with SSE)
```
Client â†’ POST /api/v1/plans/generate/stream
  â†“
Server â†’ data: {"type": "status", "message": "ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘ ì¤‘..."}\n\n
Server â†’ data: {"type": "status", "message": "AI ê³„íš ìƒì„± ì¤‘..."}\n\n
Server â†’ data: {"type": "chunk", "content": "Day 1: ê²½ë³µê¶..."}\n\n
Server â†’ data: {"type": "chunk", "content": " - Morning: ..."}\n\n
Server â†’ data: {"type": "complete", "plan": {...}}\n\n
```

---

## ğŸ› ï¸ êµ¬í˜„ ë°©ì•ˆ

### Option 1: Server-Sent Events (SSE) - **ì¶”ì²œ**

**ì¥ì :**
- âœ… HTTP/1.1 ê¸°ë°˜ (ê¸°ì¡´ ì¸í”„ë¼ ì‚¬ìš©)
- âœ… ìë™ ì¬ì—°ê²° ì§€ì›
- âœ… ë¸Œë¼ìš°ì € ë„¤ì´í‹°ë¸Œ ì§€ì› (`EventSource`)
- âœ… ë‹¨ë°©í–¥ í†µì‹ ì— ì í•© (ì„œë²„ â†’ í´ë¼ì´ì–¸íŠ¸)

**ë‹¨ì :**
- âŒ í´ë¼ì´ì–¸íŠ¸ â†’ ì„œë²„ëŠ” ë³„ë„ ìš”ì²­ í•„ìš”

### Option 2: WebSocket

**ì¥ì :**
- âœ… ì–‘ë°©í–¥ í†µì‹ 
- âœ… ì‹¤ì‹œê°„ì„± ë†’ìŒ

**ë‹¨ì :**
- âŒ ì¸í”„ë¼ ë³€ê²½ í•„ìš” (WebSocket ì§€ì›)
- âŒ ì¶”ê°€ ë³µì¡ë„

**ê²°ë¡ :** SSE ì¶”ì²œ (ì—¬í–‰ ê³„íšì€ ì„œë²„ â†’ í´ë¼ì´ì–¸íŠ¸ë§Œ í•„ìš”)

---

## ğŸ“ êµ¬í˜„ ë‹¨ê³„

### Step 1: Backend - ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

**íŒŒì¼:** `services/llm-agent/src/routes/langgraph_plans.py`

```python
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, AsyncGenerator
import json
import asyncio

router = APIRouter()
planner = LangGraphPlanner()

class LangGraphPlanRequest(BaseModel):
    location: str
    startDate: str
    endDate: str
    budget: Optional[str] = "medium"
    title: Optional[str] = None
    maxIterations: Optional[int] = 3


# ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ (í˜¸í™˜ì„± ìœ ì§€)
@router.post("/plans/generate")
async def generate_plan_with_langgraph(request: LangGraphPlanRequest):
    """ê¸°ì¡´ ë°©ì‹: ì™„ë£Œ í›„ í•œ ë²ˆì— ì‘ë‹µ"""
    # ... ê¸°ì¡´ ì½”ë“œ ìœ ì§€ ...


# ìƒˆë¡œìš´ ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸
@router.post("/plans/generate/stream")
async def generate_plan_streaming(request: LangGraphPlanRequest):
    """
    ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹: ChatGPTì²˜ëŸ¼ ì ì§„ì  ì‘ë‹µ

    Response Format (Server-Sent Events):
    data: {"type": "status", "message": "ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘ ì¤‘..."}\n\n
    data: {"type": "progress", "step": "generate_draft", "progress": 50}\n\n
    data: {"type": "chunk", "content": "Day 1: ê²½ë³µê¶..."}\n\n
    data: {"type": "complete", "plan": {...}}\n\n
    """

    async def event_generator() -> AsyncGenerator[str, None]:
        """SSE ì´ë²¤íŠ¸ ìƒì„±ê¸°"""
        try:
            # Auto-generate title
            from datetime import datetime
            if not request.title:
                start = datetime.fromisoformat(request.startDate)
                end = datetime.fromisoformat(request.endDate)
                num_days = (end - start).days + 1
                request.title = f"{request.location} {num_days}-Day Trip"

            # 1. ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘
            yield f"data: {json.dumps({'type': 'status', 'message': f'{request.location} ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘ ì¤‘...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)  # ì‹œê°ì  í”¼ë“œë°±

            # 2. AI ê³„íš ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
            yield f"data: {json.dumps({'type': 'status', 'message': 'AI ì—¬í–‰ ê³„íš ìƒì„± ì¤‘...'}, ensure_ascii=False)}\n\n"

            # LangGraph í”Œë˜ë„ˆ í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
            async for event in planner.generate_plan_streaming(
                title=request.title,
                location=request.location,
                start_date=request.startDate,
                end_date=request.endDate,
                budget=request.budget or "medium",
                max_iterations=request.maxIterations or 3
            ):
                # ê° ì´ë²¤íŠ¸ë¥¼ SSE í˜•ì‹ìœ¼ë¡œ ì „ì†¡
                yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"

            # 3. ì™„ë£Œ
            yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"

        except Exception as e:
            error_event = {
                'type': 'error',
                'message': f'ê³„íš ìƒì„± ì‹¤íŒ¨: {str(e)}'
            }
            yield f"data: {json.dumps(error_event, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Nginx buffering ë¹„í™œì„±í™”
        }
    )
```

---

### Step 2: LangGraph Planner - ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œ ì¶”ê°€

**íŒŒì¼:** `services/llm-agent/src/services/langgraph_planner.py`

```python
from typing import AsyncGenerator, Dict, Any
import json

class LangGraphPlanner:
    # ... ê¸°ì¡´ ì½”ë“œ ...

    async def generate_plan_streaming(
        self,
        title: str,
        location: str,
        start_date: str,
        end_date: str,
        budget: str = "medium",
        max_iterations: int = 3
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì—¬í–‰ ê³„íš ìƒì„±

        Yields:
            dict: ì´ë²¤íŠ¸ ê°ì²´
                - {"type": "status", "message": "..."}
                - {"type": "progress", "step": "...", "progress": 0-100}
                - {"type": "chunk", "content": "..."}
                - {"type": "complete", "plan": {...}}
        """
        from datetime import datetime

        # ë‚ ì§œ ê³„ì‚°
        start = datetime.fromisoformat(start_date)
        end = datetime.fromisoformat(end_date)
        num_days = (end - start).days + 1

        # ì´ˆê¸° ìƒíƒœ
        initial_state: PlanState = {
            "title": title or f"{location} {num_days}ì¼ ì—¬í–‰",
            "location": location,
            "start_date": start_date,
            "end_date": end_date,
            "budget": budget,
            "num_days": num_days,
            "weather_data": {},
            "places_data": {},
            "current_iteration": 0,
            "max_iterations": max_iterations,
            "plan_draft": {},
            "feedback": [],
            "final_plan": {},
            "messages": []
        }

        # Step 1: ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘
        yield {
            "type": "progress",
            "step": "gather_context",
            "message": f"{location} ë‚ ì”¨ ì •ë³´ í™•ì¸ ì¤‘...",
            "progress": 20
        }

        state = await self.gather_context_node(initial_state)

        yield {
            "type": "status",
            "message": f"ë‚ ì”¨: {state['weather_data'].get('description', 'N/A')} {state['weather_data'].get('temperature', {}).get('current', '')}Â°C",
        }

        # Step 2: AI ì´ˆì•ˆ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
        yield {
            "type": "progress",
            "step": "generate_draft",
            "message": "AI ì—¬í–‰ ê³„íš ìƒì„± ì¤‘...",
            "progress": 40
        }

        # LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        prompt = self._build_planning_prompt(state)

        if self.mock_mode or not self.llm:
            # Mock ëª¨ë“œ: ì ì§„ì ìœ¼ë¡œ ì „ì†¡
            draft = self._generate_mock_draft(state)

            # Daysë¥¼ í•˜ë‚˜ì”© ìŠ¤íŠ¸ë¦¬ë°
            for day in draft.get('days', []):
                yield {
                    "type": "chunk",
                    "content": f"Day {day['day']}: {day['location']}",
                    "day": day['day']
                }
                await asyncio.sleep(0.3)  # ì‹œê°ì  íš¨ê³¼

            state["plan_draft"] = draft

        else:
            # ì‹¤ì œ LLM ìŠ¤íŠ¸ë¦¬ë°
            messages = state["messages"] + [HumanMessage(content=prompt)]

            full_response = ""
            async for chunk in self.llm.astream(messages):
                # ì²­í¬ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡
                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                full_response += content

                yield {
                    "type": "chunk",
                    "content": content
                }

            # ì „ì²´ ì‘ë‹µ íŒŒì‹±
            draft = self._parse_llm_response(full_response, state)
            state["plan_draft"] = draft
            state["messages"].append(AIMessage(content=full_response))

        # Step 3: ê²€ì¦
        yield {
            "type": "progress",
            "step": "validate",
            "message": "ê³„íš ê²€ì¦ ì¤‘...",
            "progress": 70
        }

        state = await self.validate_plan_node(state)

        # Step 4: í•„ìš”ì‹œ ê°œì„  (iteration)
        while state["feedback"] and state["current_iteration"] < state["max_iterations"]:
            yield {
                "type": "progress",
                "step": "refine",
                "message": f"ê³„íš ê°œì„  ì¤‘ ({state['current_iteration'] + 1}/{state['max_iterations']})...",
                "progress": 80
            }

            state = await self.refine_plan_node(state)
            state["current_iteration"] += 1
            state = await self.validate_plan_node(state)

        # Step 5: ìµœì¢… ì™„ë£Œ
        yield {
            "type": "progress",
            "step": "finalize",
            "message": "ìµœì¢… ê³„íš ì™„ì„± ì¤‘...",
            "progress": 95
        }

        state = await self.finalize_plan_node(state)

        # ìµœì¢… ê²°ê³¼ ì „ì†¡
        yield {
            "type": "complete",
            "plan": state["final_plan"],
            "progress": 100
        }
```

---

### Step 3: Frontend - EventSource ì‚¬ìš©

#### React/React Native ì˜ˆì‹œ

```typescript
// src/services/planService.ts

export async function generatePlanStreaming(
  request: PlanRequest,
  onProgress: (event: StreamEvent) => void,
  onComplete: (plan: TravelPlan) => void,
  onError: (error: string) => void
) {
  const url = `${API_BASE_URL}/api/v1/plans/generate/stream`;

  try {
    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(request),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const reader = response.body?.getReader();
    const decoder = new TextDecoder();

    if (!reader) {
      throw new Error('No response body');
    }

    while (true) {
      const { done, value } = await reader.read();

      if (done) break;

      // SSE ë°ì´í„° íŒŒì‹±
      const chunk = decoder.decode(value);
      const lines = chunk.split('\n');

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.substring(6);

          try {
            const event = JSON.parse(data);

            switch (event.type) {
              case 'status':
              case 'progress':
              case 'chunk':
                onProgress(event);
                break;

              case 'complete':
                onComplete(event.plan);
                break;

              case 'error':
                onError(event.message);
                break;
            }
          } catch (e) {
            console.error('Failed to parse SSE event:', e);
          }
        }
      }
    }
  } catch (error) {
    onError(error instanceof Error ? error.message : String(error));
  }
}
```

#### React Component ì˜ˆì‹œ

```typescript
// src/components/PlanGenerator.tsx

import React, { useState } from 'react';
import { generatePlanStreaming } from '../services/planService';

export function PlanGenerator() {
  const [status, setStatus] = useState<string>('');
  const [progress, setProgress] = useState<number>(0);
  const [chunks, setChunks] = useState<string[]>([]);
  const [plan, setPlan] = useState<TravelPlan | null>(null);
  const [loading, setLoading] = useState(false);

  const handleGenerate = async () => {
    setLoading(true);
    setChunks([]);
    setPlan(null);

    await generatePlanStreaming(
      {
        location: 'Seoul',
        startDate: '2025-11-10',
        endDate: '2025-11-12',
        budget: 'medium',
      },
      // onProgress
      (event) => {
        if (event.type === 'status') {
          setStatus(event.message);
        } else if (event.type === 'progress') {
          setStatus(event.message);
          setProgress(event.progress);
        } else if (event.type === 'chunk') {
          setChunks(prev => [...prev, event.content]);
        }
      },
      // onComplete
      (completePlan) => {
        setPlan(completePlan);
        setLoading(false);
        setStatus('ì™„ë£Œ!');
      },
      // onError
      (error) => {
        console.error('Error:', error);
        setStatus(`ì˜¤ë¥˜: ${error}`);
        setLoading(false);
      }
    );
  };

  return (
    <div>
      <button onClick={handleGenerate} disabled={loading}>
        ì—¬í–‰ ê³„íš ìƒì„±
      </button>

      {loading && (
        <div>
          <div className="progress-bar">
            <div style={{ width: `${progress}%` }} />
          </div>
          <p>{status}</p>
        </div>
      )}

      {/* ì‹¤ì‹œê°„ ì²­í¬ í‘œì‹œ (ChatGPT ìŠ¤íƒ€ì¼) */}
      {chunks.length > 0 && (
        <div className="streaming-content">
          {chunks.map((chunk, i) => (
            <span key={i}>{chunk}</span>
          ))}
          <span className="cursor-blink">|</span>
        </div>
      )}

      {/* ìµœì¢… ê³„íš í‘œì‹œ */}
      {plan && (
        <div className="final-plan">
          <h2>{plan.title}</h2>
          {plan.days.map(day => (
            <div key={day.day}>
              <h3>Day {day.day}: {day.location}</h3>
              <p>{day.activity}</p>
              <p>Cost: â‚©{day.estimatedCost.toLocaleString()}</p>
            </div>
          ))}
        </div>
      )}
    </div>
  );
}
```

---

## ğŸ¨ UI/UX ê°œì„  í¬ì¸íŠ¸

### 1. ì§„í–‰ ë‹¨ê³„ë³„ ì‹œê°í™”

```typescript
const STEPS = [
  { key: 'gather_context', label: 'ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘', icon: 'ğŸŒ¤ï¸' },
  { key: 'generate_draft', label: 'AI ê³„íš ìƒì„±', icon: 'ğŸ¤–' },
  { key: 'validate', label: 'ê³„íš ê²€ì¦', icon: 'âœ…' },
  { key: 'refine', label: 'ê³„íš ê°œì„ ', icon: 'ğŸ”§' },
  { key: 'finalize', label: 'ìµœì¢… ì™„ë£Œ', icon: 'ğŸ‰' },
];

{STEPS.map((step, i) => (
  <div key={step.key} className={currentStep === step.key ? 'active' : ''}>
    {step.icon} {step.label}
  </div>
))}
```

### 2. íƒ€ì´í•‘ íš¨ê³¼ (ChatGPT ìŠ¤íƒ€ì¼)

```css
.streaming-content {
  font-family: monospace;
  white-space: pre-wrap;
}

.cursor-blink {
  animation: blink 1s infinite;
}

@keyframes blink {
  0%, 50% { opacity: 1; }
  51%, 100% { opacity: 0; }
}
```

### 3. í”„ë¡œê·¸ë ˆìŠ¤ ë°”

```tsx
<div className="progress-container">
  <div className="progress-bar" style={{ width: `${progress}%` }}>
    {progress}%
  </div>
  <p className="status-text">{status}</p>
</div>
```

---

## ğŸ“Š ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­

### 1. ì²­í¬ í¬ê¸° ì¡°ì •

```python
# ë„ˆë¬´ ì‘ì€ ì²­í¬ëŠ” ì˜¤ë²„í—¤ë“œ ì¦ê°€
MIN_CHUNK_SIZE = 10  # ìµœì†Œ 10ìì”© ì „ì†¡

buffer = ""
async for chunk in self.llm.astream(messages):
    buffer += chunk.content

    if len(buffer) >= MIN_CHUNK_SIZE:
        yield {"type": "chunk", "content": buffer}
        buffer = ""

# ë‚¨ì€ ë²„í¼ ì „ì†¡
if buffer:
    yield {"type": "chunk", "content": buffer}
```

### 2. íƒ€ì„ì•„ì›ƒ ì„¤ì •

```python
@router.post("/plans/generate/stream")
async def generate_plan_streaming(
    request: LangGraphPlanRequest,
    timeout: int = 60  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
):
    async def event_generator():
        try:
            async with asyncio.timeout(timeout):
                # ìŠ¤íŠ¸ë¦¬ë° ë¡œì§
                ...
        except asyncio.TimeoutError:
            yield f"data: {json.dumps({'type': 'error', 'message': 'ì‘ë‹µ ì‹œê°„ ì´ˆê³¼'})}\n\n"
```

### 3. ì—ëŸ¬ í•¸ë“¤ë§

```python
try:
    async for event in planner.generate_plan_streaming(...):
        yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"
except Exception as e:
    logger.error(f"Streaming error: {e}")
    yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
finally:
    # ì •ë¦¬ ì‘ì—…
    logger.info("Streaming completed")
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•

### 1. cURLë¡œ í…ŒìŠ¤íŠ¸

```bash
curl -N -X POST http://localhost:8000/api/v1/plans/generate/stream \
  -H "Content-Type: application/json" \
  -d '{
    "location": "Seoul",
    "startDate": "2025-11-10",
    "endDate": "2025-11-12",
    "budget": "medium"
  }'

# Expected Output:
# data: {"type":"status","message":"Seoul ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘ ì¤‘..."}
#
# data: {"type":"progress","step":"generate_draft","message":"AI ì—¬í–‰ ê³„íš ìƒì„± ì¤‘...","progress":40}
#
# data: {"type":"chunk","content":"Day 1: ê²½ë³µê¶"}
#
# ...
```

### 2. Python í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

```python
import requests
import json

def test_streaming():
    url = "http://localhost:8000/api/v1/plans/generate/stream"
    payload = {
        "location": "Seoul",
        "startDate": "2025-11-10",
        "endDate": "2025-11-12",
        "budget": "medium"
    }

    with requests.post(url, json=payload, stream=True) as response:
        for line in response.iter_lines():
            if line:
                decoded = line.decode('utf-8')
                if decoded.startswith('data: '):
                    event = json.loads(decoded[6:])
                    print(f"[{event['type']}] {event.get('message', event.get('content', ''))}")

if __name__ == "__main__":
    test_streaming()
```

---

## ğŸš€ ë°°í¬ ì‹œ ì£¼ì˜ì‚¬í•­

### 1. Nginx ì„¤ì • (Buffering ë¹„í™œì„±í™”)

```nginx
location /api/v1/plans/generate/stream {
    proxy_pass http://llm-agent:8000;

    # SSEë¥¼ ìœ„í•œ ì„¤ì •
    proxy_buffering off;
    proxy_cache off;
    proxy_set_header Connection '';
    proxy_http_version 1.1;
    chunked_transfer_encoding off;

    # íƒ€ì„ì•„ì›ƒ ì—°ì¥
    proxy_read_timeout 300s;
    proxy_connect_timeout 75s;
}
```

### 2. API Gateway ì„¤ì • (Spring Cloud Gateway)

```yaml
spring:
  cloud:
    gateway:
      routes:
        - id: llm-agent-stream
          uri: http://llm-agent:8000
          predicates:
            - Path=/api/v1/plans/generate/stream
          filters:
            - name: RequestTimeout
              args:
                timeout: 60s
```

### 3. ëª¨ë°”ì¼ ì•± (React Native)

```typescript
// React NativeëŠ” EventSource ë¯¸ì§€ì› â†’ Polyfill í•„ìš”
import EventSource from 'react-native-sse';

const es = new EventSource(url, {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify(request),
});

es.addEventListener('message', (event) => {
  const data = JSON.parse(event.data);
  // ì²˜ë¦¬ ë¡œì§
});

es.addEventListener('error', (error) => {
  console.error('SSE Error:', error);
  es.close();
});
```

---

## ğŸ“ˆ ì¥ë‹¨ì  ë¹„êµ

### ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹

**ì¥ì :**
- âœ… **UX ê°œì„ :** ì¦‰ê°ì ì¸ í”¼ë“œë°±, ì‚¬ìš©ì ì´íƒˆë¥  ê°ì†Œ
- âœ… **ì²´ê° ì†ë„:** ì²« ì‘ë‹µì´ ë¹ ë¥´ê²Œ ë³´ì„ (TTFB ê°œì„ )
- âœ… **ì§„í–‰ ìƒí™©:** ì‹¤ì‹œê°„ ì§„í–‰ë„ í‘œì‹œ ê°€ëŠ¥
- âœ… **íˆ¬ëª…ì„±:** ì–´ë–¤ ì‘ì—…ì´ ì§„í–‰ ì¤‘ì¸ì§€ ëª…í™•

**ë‹¨ì :**
- âŒ **ë³µì¡ë„:** êµ¬í˜„ ë° ë””ë²„ê¹… ì–´ë ¤ì›€
- âŒ **ì—ëŸ¬ ì²˜ë¦¬:** ì¤‘ê°„ì— ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬ ë³µì¡
- âŒ **ìºì‹±:** ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì€ ìºì‹± ì–´ë ¤ì›€
- âŒ **ì¸í”„ë¼:** Nginx, ë¡œë“œë°¸ëŸ°ì„œ ì„¤ì • í•„ìš”

### ê¸°ì¡´ ë°©ì‹ (í•œ ë²ˆì—)

**ì¥ì :**
- âœ… **ë‹¨ìˆœí•¨:** êµ¬í˜„ ë° ë””ë²„ê¹… ì‰¬ì›€
- âœ… **ì•ˆì •ì„±:** ì—ëŸ¬ ì²˜ë¦¬ ê°„ë‹¨
- âœ… **ìºì‹±:** Redis ìºì‹± ê°€ëŠ¥

**ë‹¨ì :**
- âŒ **UX:** ë¡œë”© ì‹œê°„ ë™ì•ˆ ëŒ€ê¸°
- âŒ **í”¼ë“œë°± ì—†ìŒ:** ì§„í–‰ ìƒí™© ëª¨ë¦„

---

## ğŸ¯ ê¶Œì¥ ì‚¬í•­

### Phase 1: í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ (ì¶”ì²œ)

1. **ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ ìœ ì§€** (`/plans/generate`)
   - ìºì‹± ê°€ëŠ¥
   - ì•ˆì •ì 
   - ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ í˜¸í™˜

2. **ìƒˆë¡œìš´ ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€** (`/plans/generate/stream`)
   - ìƒˆë¡œìš´ UX
   - ì ì§„ì  ë„ì…
   - A/B í…ŒìŠ¤íŠ¸ ê°€ëŠ¥

### Phase 2: ì„ íƒì  ì ìš©

**ìŠ¤íŠ¸ë¦¬ë° ì¶”ì²œ ìƒí™©:**
- ê¸´ ì—¬í–‰ (5ì¼ ì´ìƒ)
- ë³µì¡í•œ ìš”êµ¬ì‚¬í•­
- ì²« ë°©ë¬¸ ì‚¬ìš©ì (UX ì¤‘ìš”)

**ê¸°ì¡´ ë°©ì‹ ìœ ì§€ ìƒí™©:**
- ì§§ì€ ì—¬í–‰ (1-2ì¼)
- ë°˜ë³µ ìš”ì²­ (ìºì‹± íš¨ê³¼)
- ë¹ ë¥¸ ì‘ë‹µ í•„ìš”

---

## ğŸ“š ì°¸ê³  ìë£Œ

### LangChain Streaming
- https://python.langchain.com/docs/expression_language/streaming

### FastAPI StreamingResponse
- https://fastapi.tiangolo.com/advanced/custom-response/#streamingresponse

### Server-Sent Events (SSE)
- https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events

### Google Gemini Streaming
- https://ai.google.dev/tutorials/python_quickstart#streaming

---

**êµ¬í˜„ ì™„ë£Œ í›„ ì˜ˆìƒ íš¨ê³¼:**
- ğŸš€ ì²´ê° ì†ë„ 50% í–¥ìƒ
- ğŸ˜Š ì‚¬ìš©ì ë§Œì¡±ë„ ìƒìŠ¹
- ğŸ“‰ ì´íƒˆë¥  ê°ì†Œ
- ğŸ¯ ì°¨ë³„í™”ëœ UX

**ë‹¤ìŒ ë‹¨ê³„:** Step 1ë¶€í„° êµ¬í˜„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ?
