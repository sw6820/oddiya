# Streaming Test Frontend - User Guide ğŸš€

**Date:** 2025-11-04
**Status:** âœ… Ready for Testing

---

## ğŸ“ Access the Test Page

### URL
```
http://localhost:8000/test
```

### Service Status
- âœ… LLM Agent running on port 8000
- âœ… Redis running on port 6379
- âœ… Streaming endpoint: `/api/v1/plans/generate/stream`
- âœ… Caching enabled with 1-hour TTL

---

## ğŸ¯ How to Test

### 1. Open the Test Page
```bash
# In your browser, go to:
http://localhost:8000/test
```

### 2. Generate Your First Plan (Cache MISS)

**Default values:**
- Location: Seoul
- Start Date: Tomorrow
- End Date: 3 days from now
- Budget: Medium

**Click "Generate Travel Plan ğŸ¯"**

**What you'll see:**
```
0%   â†’ â³ Seoulì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤...
20%  â†’ âœ… ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ
30%  â†’ â³ AIê°€ ì—¬í–‰ ê³„íšì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...
35%  â†’ ğŸ’¬ [LLM chunks appearing in real-time]
40%  â†’ ğŸ’¬ Morning: ê²½ë³µê¶ (â‚©3,000)...
45%  â†’ ğŸ’¬ Afternoon: ë¶ì´Œ í•œì˜¥ë§ˆì„...
60%  â†’ âœ… 3ì¼ ì¼ì • ì´ˆì•ˆ ìƒì„± ì™„ë£Œ
70%  â†’ âœ… ê²€ì¦ ì™„ë£Œ - ë¬¸ì œ ì—†ìŒ
95%  â†’ â³ ìµœì¢… ê³„íšì„ ì™„ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...
100% â†’ âœ… ì—¬í–‰ ê³„íš ìƒì„± ì™„ë£Œ!
```

**Expected time:** 4-8 seconds
**Badge shown:** âœ¨ Newly Generated

### 3. Generate Same Plan Again (Cache HIT)

**Use the EXACT same parameters:**
- Location: Seoul
- Same start/end dates
- Same budget

**Click "Generate Travel Plan ğŸ¯" again**

**What you'll see:**
```
50%  â†’ ğŸ’¾ ì €ì¥ëœ ê³„íšì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...
100% â†’ âœ… ì €ì¥ëœ ê³„íš ë¡œë“œ ì™„ë£Œ!
```

**Expected time:** <1 second âš¡
**Badge shown:** ğŸ’¾ Cached

---

## ğŸ¨ UI Features

### Progress Bar
- Visual representation of completion (0-100%)
- Smooth transitions between steps
- Color: Purple gradient

### Status Messages (Korean)
Real-time status updates showing what the AI is doing:
- ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘ ì¤‘ (Gathering weather)
- AIê°€ ì—¬í–‰ ê³„íšì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤ (AI generating plan)
- ê³„íšì„ ê²€ì¦í•˜ê³  ìˆìŠµë‹ˆë‹¤ (Validating plan)
- ìµœì¢… ê³„íšì„ ì™„ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤ (Finalizing plan)

### LLM Chunks Display
- Shows actual text being generated by the AI
- Appears progressively (ChatGPT style)
- Auto-scrolls to show latest content
- Monospace font for better readability

### Timer
- Shows elapsed time in seconds
- Updates every 100ms
- Helps compare cached vs fresh generation

### Final Plan Display
- **Header:** Trip title + total cost
- **Badge:** Shows if cached (ğŸ’¾) or new (âœ¨)
- **Day Cards:** Each day with location, activities, cost
- **Metadata:** AI model, iterations, timestamp

---

## ğŸ§ª Test Scenarios

### Scenario 1: Fresh Generation (Cache Miss)
```javascript
Location: Seoul
Start: 2025-11-10
End: 2025-11-12
Budget: medium

Expected: 5-7 seconds, streaming progress, "Newly Generated" badge
```

### Scenario 2: Cached Retrieval (Cache Hit)
```javascript
// Same parameters as Scenario 1
Location: Seoul
Start: 2025-11-10
End: 2025-11-12
Budget: medium

Expected: <1 second, no LLM chunks, "Cached" badge
```

### Scenario 3: Different Location (Cache Miss)
```javascript
Location: Busan
Start: 2025-11-15
End: 2025-11-17
Budget: medium

Expected: 5-7 seconds, different plan content
```

### Scenario 4: Different Budget (Cache Miss)
```javascript
Location: Seoul  // Same location
Start: 2025-11-10
End: 2025-11-12
Budget: high  // Different budget = different cache key

Expected: 5-7 seconds, higher budget activities
```

---

## ğŸ“Š Cache Key Format

The cache key is generated as:
```
plan:{location}:{startDate}:{endDate}:{budget}
```

**Examples:**
```
plan:Seoul:2025-11-10:2025-11-12:medium
plan:Busan:2025-11-15:2025-11-17:low
plan:Jeju:2025-12-01:2025-12-05:high
```

**Cache Behavior:**
- âœ… Same key = Cache HIT (instant)
- âŒ Different key = Cache MISS (generate new)
- â° TTL = 1 hour (3600 seconds)

---

## ğŸ” What to Look For

### Visual Cues for Caching

**Fresh Generation:**
- Progress bar moves gradually (0â†’20â†’30â†’60â†’70â†’95â†’100)
- Status messages appear step by step
- LLM chunks visible in the chunks container
- Timer shows 5-7 seconds
- Green badge: "âœ¨ Newly Generated"

**Cached Plan:**
- Progress bar jumps directly to 50% then 100%
- Status message: "ğŸ’¾ ì €ì¥ëœ ê³„íšì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."
- No LLM chunks shown
- Status box background turns green
- Timer shows <1 second
- Blue badge: "ğŸ’¾ Cached"

---

## ğŸ› ï¸ Technical Details

### Backend Changes Applied

**1. Redis Integration** (`langgraph_plans.py`)
```python
from redis.asyncio import Redis

redis_client = Redis(
    host=os.getenv("REDIS_HOST", "localhost"),
    port=int(os.getenv("REDIS_PORT", 6379)),
    decode_responses=True
)
```

**2. Cache Check Logic**
```python
# Check cache first
cache_key = f"plan:{location}:{startDate}:{endDate}:{budget}"
cached_plan = await redis_client.get(cache_key)

if cached_plan:
    # Return immediately via streaming
    yield {"type": "complete", "plan": cached_plan, "cached": True}
else:
    # Generate new plan + save to cache
    final_plan = await generate_new_plan(...)
    await redis_client.setex(cache_key, 3600, json.dumps(final_plan))
```

**3. Static File Serving** (`main.py`)
```python
app.mount("/static", StaticFiles(directory="static"))

@app.get("/test")
async def test_page():
    return FileResponse("static/streaming-test.html")
```

### Frontend Features

**Server-Sent Events (SSE) Handler:**
```javascript
const response = await fetch('/api/v1/plans/generate/stream', {
    method: 'POST',
    body: JSON.stringify(request)
});

const reader = response.body.getReader();
while (true) {
    const {done, value} = await reader.read();
    // Parse SSE: "data: {...}\n\n"
    const event = JSON.parse(line.substring(6));
    handleEvent(event);
}
```

**Event Types Handled:**
- `status` - Status message with progress
- `progress` - Milestone reached
- `chunk` - LLM output chunk
- `complete` - Final plan ready (with cached flag)
- `error` - Error occurred

---

## ğŸ“ˆ Performance Metrics

### Expected Performance

| Metric | Fresh Generation | Cached Retrieval |
|--------|------------------|------------------|
| **Time** | 5-7 seconds | <1 second |
| **Events** | ~15-20 events | 3 events |
| **LLM Calls** | 1-2 calls | 0 calls |
| **Cost** | ~$0.01 | $0 |
| **Progress Steps** | 6-7 steps | 2 steps |

### Cost Savings

**Without Caching (100 requests):**
- Time: 500-700 seconds
- LLM calls: 100
- Cost: ~$1.00

**With Caching (100 requests, same destination):**
- Time: 6 + (99 Ã— 0.5) = ~55 seconds
- LLM calls: 1
- Cost: ~$0.01
- Savings: **99% cost reduction, 90% time reduction**

---

## ğŸ› Troubleshooting

### Issue: "Connection Error"
**Cause:** LLM Agent service not running
**Solution:**
```bash
cd /Users/wjs/cursor/oddiya/services/llm-agent
source venv/bin/activate
python main.py
```

### Issue: Test page shows "Not Found"
**Cause:** Static file not mounted
**Solution:** Check `static/streaming-test.html` exists and service restarted

### Issue: No caching happening (always fresh)
**Cause:** Redis not running
**Solution:**
```bash
# Check Redis
redis-cli ping
# Should return: PONG

# If not running:
brew services start redis
```

### Issue: Cache not expiring
**Cause:** TTL not set correctly
**Solution:** Check Redis TTL:
```bash
redis-cli TTL "plan:Seoul:2025-11-10:2025-11-12:medium"
# Should return remaining seconds (0-3600)
```

### Issue: Chunks not appearing
**Cause:** Gemini response doesn't support streaming or connection issue
**Solution:** Check logs:
```bash
tail -f /tmp/llm-agent.log
```

---

## ğŸ¯ Success Criteria

### âœ… Test Passes If:

1. **First request (fresh):**
   - Shows progressive status updates
   - Progress bar moves gradually
   - LLM chunks appear
   - Takes 5-7 seconds
   - Shows "âœ¨ Newly Generated" badge

2. **Second request (cached):**
   - Shows "ğŸ’¾ ì €ì¥ëœ ê³„íšì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."
   - Status box turns green
   - Takes <1 second
   - Shows "ğŸ’¾ Cached" badge
   - Plan content identical to first request

3. **Plan quality:**
   - Valid Korean locations and restaurant names
   - Realistic pricing (â‚©3,000-60,000 range)
   - Proper day breakdown
   - Total cost within budget guidelines

---

## ğŸ“ Test Checklist

- [ ] Service running on `http://localhost:8000`
- [ ] Redis running on port 6379
- [ ] Test page accessible at `/test`
- [ ] Fresh generation works (5-7s, streaming visible)
- [ ] Cached retrieval works (<1s, green badge)
- [ ] Different locations cache separately
- [ ] Different budgets cache separately
- [ ] UI shows progress bar correctly
- [ ] LLM chunks appear during generation
- [ ] Timer shows accurate elapsed time
- [ ] Final plan displays correctly
- [ ] Metadata shows correct info

---

## ğŸš€ Next Steps

### After Testing Locally

1. **Deploy to staging:**
   ```bash
   docker build -t oddiya/llm-agent:streaming .
   docker push oddiya/llm-agent:streaming
   kubectl set image deployment/llm-agent llm-agent=oddiya/llm-agent:streaming
   ```

2. **Update mobile app:**
   - Integrate streaming client code
   - Add progress UI components
   - Handle cached vs fresh indicators

3. **Monitor performance:**
   - Track cache hit rate
   - Monitor response times
   - Measure cost savings

---

## ğŸ“š Related Documentation

- **Implementation:** `STREAMING_IMPLEMENTATION_COMPLETE.md`
- **Guide:** `STREAMING_IMPLEMENTATION_GUIDE.md`
- **Testing:** `AI_FLOW_TEST_REPORT.md`
- **Prompts:** `PROMPT_MANAGEMENT_GUIDE.md`

---

**Ready to Test!** Open http://localhost:8000/test in your browser ğŸ‰
